"""æˆå¯¹æ¯”è¾ƒå·¥å…·"""

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
from tqdm import tqdm

from ..core.base_llm import BaseLLM
from ..core.prompt_manager import PromptManager
from ..datasets import EvalResultsDataset
from ..schemas import JudgementResult

logger = logging.getLogger(__name__)


class PairwiseJudge:
    """æˆå¯¹æ¯”è¾ƒå·¥å…·
    
    ç”¨äºæ¯”è¾ƒä¸¤ç»„è¾“å‡ºï¼ˆå¦‚åŒä¸€æ¨¡å‹çš„ä¸åŒ prompt æˆ–åŒä¸€ prompt çš„ä¸åŒæ¨¡å‹ï¼‰
    è®¾è®¡é£æ ¼ä¸ BaseTask ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨ç›¸åŒçš„ LLM æ¥å£
    """
    
    def __init__(
        self,
        csv_path_a: str,
        csv_path_b: str,
        llm: BaseLLM,
        prompt_manager: PromptManager,
        output_dir: str,
    ):
        """åˆå§‹åŒ– PairwiseJudge
        
        Args:
            csv_path_a: è¯„ä¼°ç»“æœ A çš„ CSV è·¯å¾„
            csv_path_b: è¯„ä¼°ç»“æœ B çš„ CSV è·¯å¾„
            llm: LLM å®ä¾‹
            prompt_manager: Prompt ç®¡ç†å™¨å®ä¾‹
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dataset_a = EvalResultsDataset(csv_path_a)
        self.dataset_b = EvalResultsDataset(csv_path_b)
        self.llm = llm
        self.prompt_manager = prompt_manager
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"PairwiseJudge åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - æ•°æ®é›† A: {len(self.dataset_a)} è¡Œ")
        logger.info(f"  - æ•°æ®é›† B: {len(self.dataset_b)} è¡Œ")
        logger.info(f"  - è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def build_judge_chat(
        self,
        query: str,
        gold_standard: str,
        candidate_a: str,
        candidate_b: str
    ) -> list[dict[str, str]]:
        """æ„å»ºåˆ¤åˆ« chat
        
        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            gold_standard: é‡‘æ ‡å‡†ç­”æ¡ˆ
            candidate_a: å€™é€‰A çš„è¾“å‡º
            candidate_b: å€™é€‰B çš„è¾“å‡º
            
        Returns:
            chat æ¶ˆæ¯åˆ—è¡¨
        """
        # åŠ è½½åˆ¤åˆ« prompt
        system_prompt = self.prompt_manager.load()
        
        user_content = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªå€™é€‰è¾“å‡ºï¼Œåˆ¤æ–­å“ªä¸€ä¸ªæ›´æ¥è¿‘é‡‘æ ‡å‡†ç­”æ¡ˆã€‚

## ç”¨æˆ·æŸ¥è¯¢
```
{query}
```

## é‡‘æ ‡å‡†ç­”æ¡ˆ
```json
{gold_standard}
```

## å€™é€‰A è¾“å‡º
```json
{candidate_a}
```

## å€™é€‰B è¾“å‡º
```json
{candidate_b}
```

è¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­çš„è¯„ä¼°æ ‡å‡†ï¼Œç»™å‡ºä½ çš„åˆ¤æ–­ç»“æœã€‚
        """
        
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
    
    def prepare_comparison_pairs(self) -> List[Dict[str, Any]]:
        """æŒ‰ idx é…å¯¹ä¸¤ä¸ªæ•°æ®é›†
        
        Returns:
            é…å¯¹çš„æ ·æœ¬åˆ—è¡¨
        """
        pairs = []
        
        # è·å–ä¸¤ä¸ªæ•°æ®é›†ä¸­éƒ½å­˜åœ¨çš„ idx
        indices_a = set(self.dataset_a._df['idx'].unique())
        indices_b = set(self.dataset_b._df['idx'].unique())
        common_indices = sorted(indices_a & indices_b)
        
        logger.info(f"å‘ç°å…±åŒ idx: {len(common_indices)} ä¸ª")
        
        for idx in common_indices:
            try:
                row_a = self.dataset_a.get_row_by_idx(idx)
                row_b = self.dataset_b.get_row_by_idx(idx)
            except IndexError:
                logger.debug(f"è·³è¿‡ idx={idx}ï¼Œä¸¤ä¸ªæ•°æ®é›†ä¸­ä¸åŒ¹é…")
                continue
            
            # è·³è¿‡ä¸¤è€…éƒ½å¤±è´¥çš„æƒ…å†µ
            if not row_a.get("ok", False) and not row_b.get("ok", False):
                logger.debug(f"è·³è¿‡ idx={idx}ï¼Œä¸¤ä¸ªå€™é€‰éƒ½å¤±è´¥")
                continue
            
            pair = {
                'idx': int(idx),
                'query': str(row_a.get('query', '')),
                'gold_label': str(row_a.get('gold_label', '')) if pd.notna(row_a.get('gold_label')) else "{}",
                'response_a': str(row_a.get('raw_response', '')) if pd.notna(row_a.get('raw_response')) else "{}",
                'response_b': str(row_b.get('raw_response', '')) if pd.notna(row_b.get('raw_response')) else "{}",
                'ok_a': bool(row_a.get('ok', False)),
                'ok_b': bool(row_b.get('ok', False))
            }
            pairs.append(pair)
        
        logger.info(f"å‡†å¤‡äº† {len(pairs)} ä¸ªæ¯”è¾ƒæ ·æœ¬å¯¹")
        return pairs
    
    def run_judgement(self, temperature: float = 0.0) -> Dict[str, Any]:
        """è¿è¡Œåˆ¤åˆ«æµç¨‹
        
        Args:
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹å‡†å¤‡æ¯”è¾ƒæ ·æœ¬å¯¹")
        pairs = self.prepare_comparison_pairs()
        
        if not pairs:
            logger.error("æ²¡æœ‰å¯æ¯”è¾ƒçš„æ ·æœ¬å¯¹")
            return {}
        
        logger.info(f"å¼€å§‹ LLM åˆ¤åˆ«ï¼Œå…± {len(pairs)} å¯¹")
        results = []
        
        for pair in tqdm(pairs, desc="LLM åˆ¤åˆ«ä¸­"):
            # æ„å»ºåˆ¤åˆ« chat
            chat = self.build_judge_chat(
                pair["query"],
                pair["gold_label"],
                pair["response_a"],
                pair["response_b"]
            )
            
            # è°ƒç”¨åˆ¤åˆ«æ¨¡å‹
            parsed, raw, dt = self.llm.generate_structured(
                chat,
                JudgementResult,
                temperature=temperature
            )
            
            # è®°å½•ç»“æœ
            result = {
                "idx": pair["idx"],
                "query": pair["query"],
                "winner": parsed.winner if parsed else "error",
                "confidence": parsed.confidence if parsed else 0.0,
                "reason": parsed.reason if parsed else "åˆ¤åˆ«å¤±è´¥",
                "latency_sec": dt,
                "raw_judgement": raw
            }
            
            # æ·»åŠ ç»´åº¦å¾—åˆ†
            if parsed and parsed.dimensions:
                dims = parsed.dimensions
                result.update({
                    "structure_a": dims.structure_a,
                    "structure_b": dims.structure_b,
                    "semantic_a": dims.semantic_a,
                    "semantic_b": dims.semantic_b,
                    "completeness_a": dims.completeness_a,
                    "completeness_b": dims.completeness_b,
                    "format_a": dims.format_a,
                    "format_b": dims.format_b
                })
            else:
                # å¡«å……é»˜è®¤å€¼
                result.update({
                    "structure_a": 0.0,
                    "structure_b": 0.0,
                    "semantic_a": 0.0,
                    "semantic_b": 0.0,
                    "completeness_a": 0.0,
                    "completeness_b": 0.0,
                    "format_a": 0.0,
                    "format_b": 0.0
                })
            
            results.append(result)
            
            # å»¶è¿Ÿé¿å…è¿‡è½½
            time.sleep(0.1)
        
        # åˆ†æç»“æœ
        analysis = self.analyze_results(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        results_df = pd.DataFrame(results)
        self.generate_report(analysis, results_df)
        
        return analysis
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æåˆ¤åˆ«ç»“æœ
        
        Args:
            results: åˆ¤åˆ«ç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†æç»Ÿè®¡å­—å…¸
        """
        df = pd.DataFrame(results)
        
        # ç»Ÿè®¡èƒœè´Ÿ
        a_wins = (df["winner"] == "candidate_a").sum()
        b_wins = (df["winner"] == "candidate_b").sum()
        ties = (df["winner"] == "tie").sum()
        errors = (df["winner"] == "error").sum()
        
        total_valid = a_wins + b_wins + ties
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = df["confidence"].mean()
        
        # é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence >= 0.7ï¼‰
        high_conf = df[df["confidence"] >= 0.7]
        high_conf_a_wins = (high_conf["winner"] == "candidate_a").sum()
        high_conf_b_wins = (high_conf["winner"] == "candidate_b").sum()
        
        # å„ç»´åº¦å¹³å‡å¾—åˆ†
        dim_scores = {
            "structure_a": df["structure_a"].mean(),
            "structure_b": df["structure_b"].mean(),
            "semantic_a": df["semantic_a"].mean(),
            "semantic_b": df["semantic_b"].mean(),
            "completeness_a": df["completeness_a"].mean(),
            "completeness_b": df["completeness_b"].mean(),
            "format_a": df["format_a"].mean(),
            "format_b": df["format_b"].mean()
        }
        
        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒï¼‰
        weights = {
            "structure": 0.3,
            "semantic": 0.4,
            "completeness": 0.2,
            "format": 0.1
        }
        
        a_overall = (
            dim_scores["structure_a"] * weights["structure"] +
            dim_scores["semantic_a"] * weights["semantic"] +
            dim_scores["completeness_a"] * weights["completeness"] +
            dim_scores["format_a"] * weights["format"]
        )
        
        b_overall = (
            dim_scores["structure_b"] * weights["structure"] +
            dim_scores["semantic_b"] * weights["semantic"] +
            dim_scores["completeness_b"] * weights["completeness"] +
            dim_scores["format_b"] * weights["format"]
        )
        
        analysis = {
            "total_judgements": len(df),
            "a_wins": int(a_wins),
            "b_wins": int(b_wins),
            "ties": int(ties),
            "errors": int(errors),
            "a_win_rate": a_wins / total_valid if total_valid > 0 else 0,
            "b_win_rate": b_wins / total_valid if total_valid > 0 else 0,
            "tie_rate": ties / total_valid if total_valid > 0 else 0,
            "avg_confidence": float(avg_confidence),
            "high_conf_judgements": len(high_conf),
            "high_conf_a_wins": int(high_conf_a_wins),
            "high_conf_b_wins": int(high_conf_b_wins),
            "dimension_scores": dim_scores,
            "a_overall_score": float(a_overall),
            "b_overall_score": float(b_overall)
        }
        
        logger.info(f"åˆ†æå®Œæˆ: A èƒœç‡ {analysis['a_win_rate']:.1%}, B èƒœç‡ {analysis['b_win_rate']:.1%}")
        return analysis
    
    def generate_report(self, analysis: Dict[str, Any], results_df: pd.DataFrame) -> None:
        """ç”Ÿæˆåˆ¤åˆ«æŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»Ÿè®¡
            results_df: åˆ¤åˆ«ç»“æœ DataFrame
        """
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = self.output_dir / "judgement_results.csv"
        results_df.to_csv(results_path, index=False, encoding='utf-8')
        logger.info(f"è¯¦ç»†åˆ¤åˆ«ç»“æœå·²ä¿å­˜: {results_path}")
        
        # ä¿å­˜ç»Ÿè®¡åˆ†æ
        analysis_path = self.output_dir / "judgement_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»Ÿè®¡åˆ†æå·²ä¿å­˜: {analysis_path}")
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_lines = []
        md_lines.append("# æˆå¯¹æ¯”è¾ƒåˆ¤åˆ«æŠ¥å‘Š")
        md_lines.append("")
        md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append("")
        
        md_lines.append("## ğŸ“Š èƒœè´Ÿç»Ÿè®¡")
        md_lines.append("")
        md_lines.append(f"- æ€»åˆ¤åˆ«æ•°: {analysis['total_judgements']}")
        md_lines.append(f"- **æ–¹æ¡ˆA èƒœå‡º**: {analysis['a_wins']} ({analysis['a_win_rate']*100:.1f}%)")
        md_lines.append(f"- **æ–¹æ¡ˆB èƒœå‡º**: {analysis['b_wins']} ({analysis['b_win_rate']*100:.1f}%)")
        md_lines.append(f"- **å¹³å±€**: {analysis['ties']} ({analysis['tie_rate']*100:.1f}%)")
        md_lines.append(f"- åˆ¤åˆ«å¤±è´¥: {analysis['errors']}")
        md_lines.append("")
        
        md_lines.append("## ğŸ¯ é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence â‰¥ 0.7ï¼‰")
        md_lines.append("")
        md_lines.append(f"- é«˜ç½®ä¿¡åˆ¤åˆ«æ•°: {analysis['high_conf_judgements']}")
        md_lines.append(f"- æ–¹æ¡ˆA èƒœå‡º: {analysis['high_conf_a_wins']}")
        md_lines.append(f"- æ–¹æ¡ˆB èƒœå‡º: {analysis['high_conf_b_wins']}")
        md_lines.append("")
        
        md_lines.append("## ğŸ“ˆ ç»´åº¦å¾—åˆ†å¯¹æ¯”ï¼ˆæ»¡åˆ†10åˆ†ï¼‰")
        md_lines.append("")
        md_lines.append("| ç»´åº¦ | æ–¹æ¡ˆA | æ–¹æ¡ˆB | å·®å¼‚ | ä¼˜åŠ¿æ–¹ |")
        md_lines.append("|------|-------|-------|------|--------|")
        
        dims = analysis["dimension_scores"]
        dim_pairs = [
            ("ç»“æ„å®Œæ•´æ€§", "structure_a", "structure_b"),
            ("è¯­ä¹‰å‡†ç¡®æ€§", "semantic_a", "semantic_b"),
            ("ä¿¡æ¯å®Œæ•´åº¦", "completeness_a", "completeness_b"),
            ("æ ¼å¼è§„èŒƒæ€§", "format_a", "format_b")
        ]
        
        for label, a_key, b_key in dim_pairs:
            a_val = dims[a_key]
            b_val = dims[b_key]
            diff = a_val - b_val
            winner = "âœ… A" if diff > 0.5 else "âœ… B" if diff < -0.5 else "â– ç›¸å½“"
            md_lines.append(f"| {label} | {a_val:.2f} | {b_val:.2f} | {diff:+.2f} | {winner} |")
        
        md_lines.append("")
        
        md_lines.append("## ğŸ† ç»¼åˆå¾—åˆ†")
        md_lines.append("")
        md_lines.append(f"- **æ–¹æ¡ˆA**: {analysis['a_overall_score']:.2f} åˆ†")
        md_lines.append(f"- **æ–¹æ¡ˆB**: {analysis['b_overall_score']:.2f} åˆ†")
        md_lines.append(f"- **å·®å¼‚**: {analysis['a_overall_score'] - analysis['b_overall_score']:+.2f} åˆ†")
        md_lines.append("")
        
        md_lines.append("## ğŸ’¡ ç»“è®º")
        md_lines.append("")
        if analysis['a_win_rate'] > 0.6:
            md_lines.append(f"âœ… **æ–¹æ¡ˆA æ˜¾è‘—ä¼˜äºæ–¹æ¡ˆB**")
            md_lines.append(f"   - èƒœç‡: {analysis['a_win_rate']*100:.1f}%")
            md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['a_overall_score']:.2f} vs {analysis['b_overall_score']:.2f}")
        elif analysis['b_win_rate'] > 0.6:
            md_lines.append(f"âœ… **æ–¹æ¡ˆB æ˜¾è‘—ä¼˜äºæ–¹æ¡ˆA**")
            md_lines.append(f"   - èƒœç‡: {analysis['b_win_rate']*100:.1f}%")
            md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['b_overall_score']:.2f} vs {analysis['a_overall_score']:.2f}")
        else:
            md_lines.append(f"â– **ä¸¤ä¸ªæ–¹æ¡ˆè¡¨ç°ç›¸å½“**")
            md_lines.append(f"   - æ–¹æ¡ˆA èƒœç‡: {analysis['a_win_rate']*100:.1f}%")
            md_lines.append(f"   - æ–¹æ¡ˆB èƒœç‡: {analysis['b_win_rate']*100:.1f}%")
        
        md_lines.append("")
        md_lines.append(f"å¹³å‡ç½®ä¿¡åº¦: {analysis['avg_confidence']:.2f}")
        md_lines.append("")
        
        # ä¿å­˜ Markdown æŠ¥å‘Š
        md_path = self.output_dir / "judgement_report.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(md_lines))
        logger.info(f"Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")
