"""æˆå¯¹æ¯”è¾ƒå·¥å…·"""

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
from tqdm import tqdm

from ..core.base_llm import BaseLLM
from ..core.prompt_manager import PromptManager
from ..datasets import EvalResultsDataset
from ..schemas import JudgementResult

logger = logging.getLogger(__name__)


class PairwiseJudge:
    """æˆå¯¹æ¯”è¾ƒå·¥å…·
    
    ç”¨äºæ¯”è¾ƒä¸¤ç»„è¾“å‡ºï¼ˆå¦‚åŒä¸€æ¨¡å‹çš„ä¸åŒ prompt æˆ–åŒä¸€ prompt çš„ä¸åŒæ¨¡å‹ï¼‰
    è®¾è®¡é£æ ¼ä¸ BaseTask ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨ç›¸åŒçš„ LLM æ¥å£
    """
    
    def __init__(
        self,
        eval_results_path: str,
        llm: BaseLLM,
        prompt_manager: PromptManager,
        output_dir: str,
    ):
        """åˆå§‹åŒ– PairwiseJudge
        
        Args:
            eval_results_path: eval_results.csv æ–‡ä»¶è·¯å¾„
            llm: LLM å®ä¾‹
            prompt_manager: Prompt ç®¡ç†å™¨å®ä¾‹
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dataset = EvalResultsDataset(eval_results_path)
        self.llm = llm
        self.prompt_manager = prompt_manager
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"PairwiseJudge åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def build_judge_chat(
        self,
        query: str,
        gold_standard: str,
        candidate_a: str,
        candidate_b: str
    ) -> list[dict[str, str]]:
        """æ„å»ºåˆ¤åˆ« chat
        
        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            gold_standard: é‡‘æ ‡å‡†ç­”æ¡ˆ
            candidate_a: å€™é€‰A çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯ newï¼‰
            candidate_b: å€™é€‰B çš„è¾“å‡ºï¼ˆé€šå¸¸æ˜¯ oldï¼‰
            
        Returns:
            chat æ¶ˆæ¯åˆ—è¡¨
        """
        # åŠ è½½åˆ¤åˆ« prompt
        system_prompt = self.prompt_manager.load()
        
        user_content = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªå€™é€‰è¾“å‡ºï¼Œåˆ¤æ–­å“ªä¸€ä¸ªæ›´æ¥è¿‘é‡‘æ ‡å‡†ç­”æ¡ˆã€‚

## ç”¨æˆ·æŸ¥è¯¢
```
{query}
```

## é‡‘æ ‡å‡†ç­”æ¡ˆ
```json
{gold_standard}
```

## å€™é€‰A è¾“å‡º
```json
{candidate_a}
```

## å€™é€‰B è¾“å‡º
```json
{candidate_b}
```

è¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­çš„è¯„ä¼°æ ‡å‡†ï¼Œç»™å‡ºä½ çš„åˆ¤æ–­ç»“æœã€‚
        """
        
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
    
    def run_judgement(self, temperature: float = 0.0) -> Dict[str, Any]:
        """è¿è¡Œåˆ¤åˆ«æµç¨‹
        
        Args:
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹å‡†å¤‡æ¯”è¾ƒæ ·æœ¬å¯¹")
        pairs = self.dataset.prepare_comparison_pairs()
        
        if not pairs:
            logger.error("æ²¡æœ‰å¯æ¯”è¾ƒçš„æ ·æœ¬å¯¹")
            return {}
        
        logger.info(f"å¼€å§‹ LLM åˆ¤åˆ«ï¼Œå…± {len(pairs)} å¯¹")
        results = []
        
        for pair in tqdm(pairs, desc="LLM åˆ¤åˆ«ä¸­"):
            # æ„å»ºåˆ¤åˆ« chat
            chat = self.build_judge_chat(
                pair["query"],
                pair["gold_label"],
                pair["new_response"],
                pair["old_response"]
            )
            
            # è°ƒç”¨åˆ¤åˆ«æ¨¡å‹
            parsed, raw, dt = self.llm.generate_structured(
                chat,
                JudgementResult,
                temperature=temperature
            )
            
            # è®°å½•ç»“æœ
            result = {
                "idx": pair["idx"],
                "query": pair["query"],
                "winner": parsed.winner if parsed else "error",
                "confidence": parsed.confidence if parsed else 0.0,
                "reason": parsed.reason if parsed else "åˆ¤åˆ«å¤±è´¥",
                "latency_sec": dt,
                "raw_judgement": raw
            }
            
            # æ·»åŠ ç»´åº¦å¾—åˆ†
            if parsed and parsed.dimensions:
                dims = parsed.dimensions
                result.update({
                    "structure_a_new": dims.structure_a,
                    "structure_b_old": dims.structure_b,
                    "semantic_a_new": dims.semantic_a,
                    "semantic_b_old": dims.semantic_b,
                    "completeness_a_new": dims.completeness_a,
                    "completeness_b_old": dims.completeness_b,
                    "format_a_new": dims.format_a,
                    "format_b_old": dims.format_b
                })
            else:
                # å¡«å……é»˜è®¤å€¼
                result.update({
                    "structure_a_new": 0.0,
                    "structure_b_old": 0.0,
                    "semantic_a_new": 0.0,
                    "semantic_b_old": 0.0,
                    "completeness_a_new": 0.0,
                    "completeness_b_old": 0.0,
                    "format_a_new": 0.0,
                    "format_b_old": 0.0
                })
            
            results.append(result)
            
            # å»¶è¿Ÿé¿å…è¿‡è½½
            time.sleep(0.1)
        
        # åˆ†æç»“æœ
        analysis = self.analyze_results(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        results_df = pd.DataFrame(results)
        self.generate_report(analysis, results_df)
        
        return analysis
    
    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†æåˆ¤åˆ«ç»“æœ
        
        Args:
            results: åˆ¤åˆ«ç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†æç»Ÿè®¡å­—å…¸
        """
        df = pd.DataFrame(results)
        
        # ç»Ÿè®¡èƒœè´Ÿ
        new_wins = (df["winner"] == "candidate_a").sum()
        old_wins = (df["winner"] == "candidate_b").sum()
        ties = (df["winner"] == "tie").sum()
        errors = (df["winner"] == "error").sum()
        
        total_valid = new_wins + old_wins + ties
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = df["confidence"].mean()
        
        # é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence >= 0.7ï¼‰
        high_conf = df[df["confidence"] >= 0.7]
        high_conf_new_wins = (high_conf["winner"] == "candidate_a").sum()
        high_conf_old_wins = (high_conf["winner"] == "candidate_b").sum()
        
        # å„ç»´åº¦å¹³å‡å¾—åˆ†
        dim_scores = {
            "new_structure": df["structure_a_new"].mean(),
            "old_structure": df["structure_b_old"].mean(),
            "new_semantic": df["semantic_a_new"].mean(),
            "old_semantic": df["semantic_b_old"].mean(),
            "new_completeness": df["completeness_a_new"].mean(),
            "old_completeness": df["completeness_b_old"].mean(),
            "new_format": df["format_a_new"].mean(),
            "old_format": df["format_b_old"].mean()
        }
        
        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒï¼‰
        weights = {
            "structure": 0.3,
            "semantic": 0.4,
            "completeness": 0.2,
            "format": 0.1
        }
        
        new_overall = (
            dim_scores["new_structure"] * weights["structure"] +
            dim_scores["new_semantic"] * weights["semantic"] +
            dim_scores["new_completeness"] * weights["completeness"] +
            dim_scores["new_format"] * weights["format"]
        )
        
        old_overall = (
            dim_scores["old_structure"] * weights["structure"] +
            dim_scores["old_semantic"] * weights["semantic"] +
            dim_scores["old_completeness"] * weights["completeness"] +
            dim_scores["old_format"] * weights["format"]
        )
        
        analysis = {
            "total_judgements": len(df),
            "new_wins": int(new_wins),
            "old_wins": int(old_wins),
            "ties": int(ties),
            "errors": int(errors),
            "new_win_rate": new_wins / total_valid if total_valid > 0 else 0,
            "old_win_rate": old_wins / total_valid if total_valid > 0 else 0,
            "tie_rate": ties / total_valid if total_valid > 0 else 0,
            "avg_confidence": float(avg_confidence),
            "high_conf_judgements": len(high_conf),
            "high_conf_new_wins": int(high_conf_new_wins),
            "high_conf_old_wins": int(high_conf_old_wins),
            "dimension_scores": dim_scores,
            "new_overall_score": float(new_overall),
            "old_overall_score": float(old_overall)
        }
        
        logger.info(f"åˆ†æå®Œæˆ: NEW èƒœç‡ {analysis['new_win_rate']:.1%}, OLD èƒœç‡ {analysis['old_win_rate']:.1%}")
        return analysis
    
    def generate_report(self, analysis: Dict[str, Any], results_df: pd.DataFrame) -> None:
        """ç”Ÿæˆåˆ¤åˆ«æŠ¥å‘Š
        
        Args:
            analysis: åˆ†æç»Ÿè®¡
            results_df: åˆ¤åˆ«ç»“æœ DataFrame
        """
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_path = self.output_dir / "llm_judgement_results.csv"
        results_df.to_csv(results_path, index=False, encoding='utf-8')
        logger.info(f"è¯¦ç»†åˆ¤åˆ«ç»“æœå·²ä¿å­˜: {results_path}")
        
        # ä¿å­˜ç»Ÿè®¡åˆ†æ
        analysis_path = self.output_dir / "llm_judgement_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»Ÿè®¡åˆ†æå·²ä¿å­˜: {analysis_path}")
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_lines = []
        md_lines.append("# LLM åˆ¤åˆ«æŠ¥å‘Š")
        md_lines.append("")
        md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append("")
        
        md_lines.append("## ğŸ“Š èƒœè´Ÿç»Ÿè®¡")
        md_lines.append("")
        md_lines.append(f"- æ€»åˆ¤åˆ«æ•°: {analysis['total_judgements']}")
        md_lines.append(f"- **NEW èƒœå‡º**: {analysis['new_wins']} ({analysis['new_win_rate']*100:.1f}%)")
        md_lines.append(f"- **OLD èƒœå‡º**: {analysis['old_wins']} ({analysis['old_win_rate']*100:.1f}%)")
        md_lines.append(f"- **å¹³å±€**: {analysis['ties']} ({analysis['tie_rate']*100:.1f}%)")
        md_lines.append(f"- åˆ¤åˆ«å¤±è´¥: {analysis['errors']}")
        md_lines.append("")
        
        md_lines.append("## ğŸ¯ é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence â‰¥ 0.7ï¼‰")
        md_lines.append("")
        md_lines.append(f"- é«˜ç½®ä¿¡åˆ¤åˆ«æ•°: {analysis['high_conf_judgements']}")
        md_lines.append(f"- NEW èƒœå‡º: {analysis['high_conf_new_wins']}")
        md_lines.append(f"- OLD èƒœå‡º: {analysis['high_conf_old_wins']}")
        md_lines.append("")
        
        md_lines.append("## ğŸ“ˆ ç»´åº¦å¾—åˆ†å¯¹æ¯”ï¼ˆæ»¡åˆ†10åˆ†ï¼‰")
        md_lines.append("")
        md_lines.append("| ç»´åº¦ | NEW | OLD | å·®å¼‚ | ä¼˜åŠ¿æ–¹ |")
        md_lines.append("|------|-----|-----|------|--------|")
        
        dims = analysis["dimension_scores"]
        dim_pairs = [
            ("ç»“æ„å®Œæ•´æ€§", "new_structure", "old_structure"),
            ("è¯­ä¹‰å‡†ç¡®æ€§", "new_semantic", "old_semantic"),
            ("ä¿¡æ¯å®Œæ•´åº¦", "new_completeness", "old_completeness"),
            ("æ ¼å¼è§„èŒƒæ€§", "new_format", "old_format")
        ]
        
        for label, new_key, old_key in dim_pairs:
            new_val = dims[new_key]
            old_val = dims[old_key]
            diff = new_val - old_val
            winner = "âœ… NEW" if diff > 0.5 else "âœ… OLD" if diff < -0.5 else "â– ç›¸å½“"
            md_lines.append(f"| {label} | {new_val:.2f} | {old_val:.2f} | {diff:+.2f} | {winner} |")
        
        md_lines.append("")
        
        md_lines.append("## ğŸ† ç»¼åˆå¾—åˆ†")
        md_lines.append("")
        md_lines.append(f"- **NEW Prompt**: {analysis['new_overall_score']:.2f} åˆ†")
        md_lines.append(f"- **OLD Prompt**: {analysis['old_overall_score']:.2f} åˆ†")
        md_lines.append(f"- **å·®å¼‚**: {analysis['new_overall_score'] - analysis['old_overall_score']:+.2f} åˆ†")
        md_lines.append("")
        
        md_lines.append("## ğŸ’¡ ç»“è®º")
        md_lines.append("")
        if analysis['new_win_rate'] > 0.6:
            md_lines.append(f"âœ… **NEW Prompt æ˜¾è‘—ä¼˜äº OLD Prompt**")
            md_lines.append(f"   - èƒœç‡: {analysis['new_win_rate']*100:.1f}%")
            md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['new_overall_score']:.2f} vs {analysis['old_overall_score']:.2f}")
        elif analysis['old_win_rate'] > 0.6:
            md_lines.append(f"âŒ **OLD Prompt ä¼˜äº NEW Prompt**")
            md_lines.append(f"   - èƒœç‡: {analysis['old_win_rate']*100:.1f}%")
            md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['old_overall_score']:.2f} vs {analysis['new_overall_score']:.2f}")
        else:
            md_lines.append(f"â– **ä¸¤è€…è¡¨ç°ç›¸å½“**")
            md_lines.append(f"   - NEW èƒœç‡: {analysis['new_win_rate']*100:.1f}%")
            md_lines.append(f"   - OLD èƒœç‡: {analysis['old_win_rate']*100:.1f}%")
        
        md_lines.append("")
        md_lines.append(f"å¹³å‡ç½®ä¿¡åº¦: {analysis['avg_confidence']:.2f}")
        md_lines.append("")
        
        # ä¿å­˜ Markdown æŠ¥å‘Š
        md_path = self.output_dir / "llm_judgement_report.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(md_lines))
        logger.info(f"Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")
