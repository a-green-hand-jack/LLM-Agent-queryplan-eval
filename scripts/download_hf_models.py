#!/usr/bin/env python3
"""
ç®€å•çš„ Hugging Face æ¨¡å‹ä¸‹è½½è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python download_hf_models.py <æ¨¡å‹åç§°>
    
ç¤ºä¾‹:
    python download_hf_models.py Qwen/Qwen2.5-14B-Instruct
    python download_hf_models.py microsoft/DialoGPT-small
    python download_hf_models.py distilbert-base-uncased
"""

import sys
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def download_model(model_name: str, download_only: bool = False) -> bool:
    """ä¸‹è½½æŒ‡å®šçš„ Hugging Face æ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚ "Qwen/Qwen2.5-14B-Instruct"
        download_only: æ˜¯å¦ä»…ä¸‹è½½ä¸åŠ è½½åˆ°å†…å­˜
        
    Returns:
        bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
    """
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        logging.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        
        # ä¸‹è½½åˆ†è¯å™¨
        logging.info("ğŸ“¥ ä¸‹è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        logging.info(f"âœ… åˆ†è¯å™¨ä¸‹è½½å®Œæˆ (è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size})")
        
        if download_only:
            logging.info("ğŸ“¥ ä»…ä¸‹è½½æ¨¡å‹æƒé‡...")
            # ä»…ä¸‹è½½æ¨¡å‹æƒé‡ï¼Œä¸åŠ è½½åˆ°å†…å­˜
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto",
                device_map=None,  # ä¸è‡ªåŠ¨åˆ†é…è®¾å¤‡
                low_cpu_mem_usage=True
            )
            # ç«‹å³é‡Šæ”¾å†…å­˜
            del model
            logging.info("âœ… æ¨¡å‹æƒé‡ä¸‹è½½å®Œæˆ")
        else:
            logging.info("ğŸ“¥ ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            # ç®€å•æµ‹è¯•
            logging.info("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
            messages = [{"role": "user", "content": "Hello!"}]
            inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"
            )
            
            # ç”Ÿæˆç®€çŸ­å›å¤
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            outputs = model.generate(
                **inputs, 
                max_new_tokens=20,
                do_sample=False,
                temperature=0.7
            )
            
            response = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[-1]:], 
                skip_special_tokens=True
            )
            logging.info(f"ğŸ¤– æ¨¡å‹å›å¤: {response}")
            
            # é‡Šæ”¾å†…å­˜
            del model
        
        logging.info(f"ğŸ‰ æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆï¼")
        return True
        
    except Exception as e:
        logging.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def show_usage():
    """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜"""
    print("""
ğŸ¤– Hugging Face æ¨¡å‹ä¸‹è½½å·¥å…·

ä½¿ç”¨æ–¹æ³•:
    python download_hf_models.py <æ¨¡å‹åç§°> [é€‰é¡¹]

ç¤ºä¾‹:
    # ä¸‹è½½å¹¶æµ‹è¯•æ¨¡å‹
    python download_hf_models.py Qwen/Qwen2.5-14B-Instruct
    
    # ä»…ä¸‹è½½æ¨¡å‹æƒé‡ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰
    python download_hf_models.py microsoft/DialoGPT-small --download-only
    
    # ä¸‹è½½å…¶ä»–ç±»å‹æ¨¡å‹
    python download_hf_models.py distilbert-base-uncased

é€‰é¡¹:
    --download-only    ä»…ä¸‹è½½æ¨¡å‹æƒé‡ï¼Œä¸åŠ è½½åˆ°å†…å­˜ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    --help, -h         æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

å¸¸ç”¨æ¨¡å‹æ¨è:
    # å¤§è¯­è¨€æ¨¡å‹
    Qwen/Qwen2.5-14B-Instruct
    Qwen/Qwen2.5-7B-Instruct
    microsoft/DialoGPT-small
    
    # æ–‡æœ¬å¤„ç†
    distilbert-base-uncased
    bert-base-uncased
    
    # å¤šæ¨¡æ€
    microsoft/git-base
    """)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        show_usage()
        return
    
    model_name = sys.argv[1]
    download_only = "--download-only" in sys.argv
    
    if model_name in ["--help", "-h"]:
        show_usage()
        return
    
    logging.info(f"ğŸš€ å¼€å§‹å¤„ç†æ¨¡å‹: {model_name}")
    logging.info("ğŸ“ ç¼“å­˜ä½ç½®: /ibex/user/wuj0c/cache/HF")
    
    success = download_model(model_name, download_only)
    
    if success:
        logging.info("âœ… ä»»åŠ¡å®Œæˆï¼")
    else:
        logging.error("âŒ ä»»åŠ¡å¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main()