#!/usr/bin/env python3
"""Prompt æ•ˆæœåˆ†æè„šæœ¬

åŸºäº docs/ai/02_analsys_prompt.md ä¸­å®šä¹‰çš„åˆ†ææ–¹æ³•ï¼Œå¯¹æ¯”æ–°æ—§ prompt çš„æ•ˆæœã€‚

ä½¿ç”¨æ–¹å¼:
    uv run python scripts/analyze_prompts.py outputs/v4/eval_results.csv
    uv run python scripts/analyze_prompts.py outputs/v4/eval_results.csv --outdir outputs/v4
"""

import sys
from pathlib import Path

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))

import json
import logging
from typing import Any
from difflib import SequenceMatcher

import pandas as pd
import numpy as np
from argparse import ArgumentParser

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# ============================================================================
# 1. ç»“æ„åŒ–ç¨‹åº¦åˆ†æ
# ============================================================================

def check_json_validity(raw_response: str) -> dict[str, Any]:
    """æ£€æŸ¥ raw_response çš„ JSON æœ‰æ•ˆæ€§
    
    Args:
        raw_response: åŸå§‹å“åº”å­—ç¬¦ä¸²
        
    Returns:
        åŒ…å«æœ‰æ•ˆæ€§ä¿¡æ¯çš„å­—å…¸
    """
    try:
        obj = json.loads(raw_response.strip())
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‹’ç­”
        if isinstance(obj, dict) and obj.get("refuse"):
            return {
                "valid": True,
                "type": "refuse",
                "fields_complete": "reason" in obj or "refuse_reason" in obj
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è®¡åˆ’å¯¹è±¡ï¼ˆæ–°æ ¼å¼ï¼‰
        if isinstance(obj, dict) and "plans" in obj:
            plans = obj["plans"]
            if not plans:
                return {"valid": True, "type": "empty_plan", "completeness": 1.0}
            
            required_fields = {"domain", "sub", "is_personal", "time", "food"}
            complete_items = sum(
                1 for item in plans 
                if isinstance(item, dict) and required_fields.issubset(item.keys())
            )
            completeness = complete_items / len(plans)
            
            return {
                "valid": True,
                "type": "plans",
                "completeness": completeness,
                "n_plans": len(plans)
            }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è®¡åˆ’æ•°ç»„ï¼ˆæ—§æ ¼å¼ï¼‰
        if isinstance(obj, list):
            if not obj:
                return {"valid": True, "type": "empty_plan", "completeness": 1.0}
            
            required_fields = {"domain", "sub", "is_personal", "time", "food", "query"}
            complete_items = sum(
                1 for item in obj 
                if isinstance(item, dict) and required_fields.issubset(item.keys())
            )
            completeness = complete_items / len(obj)
            
            return {
                "valid": True,
                "type": "plans",
                "completeness": completeness,
                "n_plans": len(obj)
            }
        
        return {"valid": False, "reason": "unexpected_structure"}
    except json.JSONDecodeError as e:
        return {"valid": False, "reason": f"json_decode_error: {str(e)[:50]}"}
    except Exception as e:
        return {"valid": False, "reason": f"unexpected_error: {str(e)[:50]}"}


def analyze_structure(df: pd.DataFrame, variant: str) -> dict[str, Any]:
    """åˆ†æç»“æ„åŒ–ç¨‹åº¦
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        variant: prompt å˜ä½“ ('new' æˆ– 'old')
        
    Returns:
        ç»“æ„åŒ–ç¨‹åº¦åˆ†æç»“æœ
    """
    dv = df[df["variant"] == variant]
    
    # æ£€æŸ¥æ¯ä¸ªå“åº”çš„ JSON æœ‰æ•ˆæ€§
    validity_results = []
    for raw_resp in dv["raw_response"]:
        validity_results.append(check_json_validity(str(raw_resp)))
    
    # ç»Ÿè®¡æŒ‡æ ‡
    valid_count = sum(1 for r in validity_results if r.get("valid", False))
    total_count = len(validity_results)
    
    # å­—æ®µå®Œæ•´æ€§
    completeness_scores = [
        r.get("completeness", 0) 
        for r in validity_results 
        if r.get("valid", False) and r.get("type") == "plans"
    ]
    avg_completeness = np.mean(completeness_scores) if completeness_scores else 0
    
    return {
        "variant": variant,
        "json_validity": valid_count / total_count if total_count > 0 else 0,
        "avg_completeness": avg_completeness,
        "total_responses": total_count,
        "valid_responses": valid_count
    }


# ============================================================================
# 2. é‡‘æ ‡ç­¾åŒ¹é…åº¦åˆ†æ
# ============================================================================

def normalize_json(obj: Any) -> Any:
    """è§„èŒƒåŒ– JSON å¯¹è±¡ç”¨äºæ¯”è¾ƒ"""
    if isinstance(obj, dict):
        # å¦‚æœæ˜¯æ–°æ ¼å¼çš„åŒ…è£…å¯¹è±¡ï¼Œæå– plans
        if "plans" in obj:
            return normalize_json(obj["plans"])
        # é€’å½’è§„èŒƒåŒ–å­—å…¸
        return {k: normalize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [normalize_json(item) for item in obj]
    else:
        return obj


def compare_with_gold(raw_response: str, gold_label: str) -> dict[str, Any]:
    """å¯¹æ¯” raw_response ä¸ gold_label
    
    Args:
        raw_response: åŸå§‹å“åº”
        gold_label: é‡‘æ ‡ç­¾
        
    Returns:
        åŒ¹é…åº¦åˆ†æç»“æœ
    """
    try:
        # è§£æé¢„æµ‹ç»“æœ
        try:
            pred = json.loads(raw_response.strip())
            pred = normalize_json(pred)
        except Exception:
            return {"exact_match": False, "key_field_match": False, "valid": False, "similarity": 0.0}
        
        # è§£æé‡‘æ ‡ç­¾
        try:
            if str(gold_label).strip() == "REFUSE" or pd.isna(gold_label):
                gold = {"refuse": True}
            else:
                gold = json.loads(str(gold_label).strip())
                gold = normalize_json(gold)
        except Exception:
            return {"exact_match": False, "key_field_match": False, "valid": False, "similarity": 0.0}
        
        # 1. å®Œå…¨åŒ¹é…
        exact_match = (pred == gold)
        
        # 2. å…³é”®å­—æ®µåŒ¹é…
        key_field_match = False
        if isinstance(pred, list) and isinstance(gold, list):
            if len(pred) == len(gold):
                key_field_match = all(
                    p.get("domain") == g.get("domain") and 
                    p.get("is_personal") == g.get("is_personal")
                    for p, g in zip(pred, gold)
                )
        elif isinstance(pred, dict) and isinstance(gold, dict):
            # éƒ½æ˜¯æ‹’ç­”
            if pred.get("refuse") and gold.get("refuse"):
                key_field_match = True
        
        # 3. ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(
            None,
            json.dumps(pred, ensure_ascii=False, sort_keys=True),
            json.dumps(gold, ensure_ascii=False, sort_keys=True)
        ).ratio()
        
        return {
            "exact_match": exact_match,
            "key_field_match": key_field_match,
            "similarity": similarity,
            "valid": True
        }
    except Exception as e:
        logging.debug(f"æ¯”è¾ƒå¤±è´¥: {str(e)[:50]}")
        return {
            "exact_match": False,
            "key_field_match": False,
            "similarity": 0.0,
            "valid": False,
            "error": str(e)[:50]
        }


def analyze_gold_match(df: pd.DataFrame, variant: str) -> dict[str, Any]:
    """åˆ†æé‡‘æ ‡ç­¾åŒ¹é…åº¦
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        variant: prompt å˜ä½“
        
    Returns:
        åŒ¹é…åº¦åˆ†æç»“æœ
    """
    dv = df[df["variant"] == variant]
    
    # å¯¹æ¯è¡Œè¿›è¡Œå¯¹æ¯”
    match_results = []
    for _, row in dv.iterrows():
        result = compare_with_gold(str(row["raw_response"]), str(row["gold_label"]))
        match_results.append(result)
    
    # ç»Ÿè®¡æŒ‡æ ‡
    valid_comparisons = [r for r in match_results if r.get("valid", False)]
    exact_matches = sum(1 for r in valid_comparisons if r.get("exact_match", False))
    key_field_matches = sum(1 for r in valid_comparisons if r.get("key_field_match", False))
    similarities = [r.get("similarity", 0) for r in valid_comparisons]
    
    return {
        "variant": variant,
        "exact_match_rate": exact_matches / len(valid_comparisons) if valid_comparisons else 0,
        "key_field_match_rate": key_field_matches / len(valid_comparisons) if valid_comparisons else 0,
        "avg_similarity": np.mean(similarities) if similarities else 0,
        "total_comparisons": len(dv),
        "valid_comparisons": len(valid_comparisons)
    }


# ============================================================================
# 3. å¤šæ ·æ€§ä¸è¦†ç›–ç‡åˆ†æ
# ============================================================================

def analyze_diversity(df: pd.DataFrame, variant: str) -> dict[str, Any]:
    """åˆ†æè¾“å‡ºå¤šæ ·æ€§
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        variant: prompt å˜ä½“
        
    Returns:
        å¤šæ ·æ€§åˆ†æç»“æœ
    """
    dv = df[df["variant"] == variant]
    
    # æ€»æŸ¥è¯¢æ•°
    n_queries = dv["idx"].nunique()
    
    # ä¸åŒçš„ raw_response ç§ç±»
    unique_raw = dv["raw_response"].nunique()
    diversity_rate = unique_raw / len(dv) if len(dv) > 0 else 0
    
    # æ‹’ç­”æ¯”ä¾‹
    refuse_count = (dv["type"] == "refuse").sum()
    refuse_rate = refuse_count / len(dv) if len(dv) > 0 else 0
    
    # è®¡åˆ’å¹³å‡é•¿åº¦ï¼ˆä»…ç»Ÿè®¡éæ‹’ç­”çš„ï¼‰
    plans_dv = dv[dv["type"] == "plans"]
    avg_plan_length = (
        plans_dv["n_plans"].mean() 
        if len(plans_dv) > 0 else 0
    )
    
    return {
        "variant": variant,
        "n_queries": n_queries,
        "unique_raw_responses": unique_raw,
        "diversity_rate": diversity_rate,
        "refuse_rate": refuse_rate,
        "avg_plan_length": avg_plan_length
    }


# ============================================================================
# 4. é²æ£’æ€§åˆ†æ
# ============================================================================

def analyze_robustness(df: pd.DataFrame, variant: str, 
                       valid_domains: set[str] | None = None) -> dict[str, Any]:
    """åˆ†æé²æ£’æ€§
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        variant: prompt å˜ä½“
        valid_domains: æœ‰æ•ˆçš„ domain é›†åˆ
        
    Returns:
        é²æ£’æ€§åˆ†æç»“æœ
    """
    dv = df[df["variant"] == variant]
    
    # 1. å¤±è´¥ç‡
    failure_mask = dv["ok"] == False
    failure_rate = failure_mask.sum() / len(dv) if len(dv) > 0 else 0
    
    # 2. å¹»è§‰ç‡ï¼ˆè¾“å‡ºä¸å­˜åœ¨çš„ domainï¼‰
    if valid_domains is None:
        valid_domains = {
            "ä½“æ¸©", "å‡è„‚", "å¿ƒè„å¥åº·", "æƒ…ç»ªå¥åº·", "ç”Ÿç†å¥åº·",
            "è¡€å‹", "è¡€æ°§é¥±å’Œåº¦", "è¡€ç³–", "ç¡çœ ", "åˆç¡", "æ­¥æ•°",
            "æ´»åŠ›ä¸‰ç¯", "å¾®ä½“æ£€", "é¥®é£Ÿ", "è·‘æ­¥", "éª‘è¡Œ", "æ­¥è¡Œå¾’æ­¥",
            "æ¸¸æ³³", "ç™»å±±", "è·³ç»³", "ç‘œä¼½", "æ™®æ‹‰æ", "åˆ’èˆ¹æœº", "å…¶ä»–"
        }
    
    hallucination_count = 0
    total_plans = 0
    for _, row in dv.iterrows():
        if row["type"] == "plans" and pd.notna(row["parsed"]):
            try:
                parsed = json.loads(str(row["parsed"]))
                # å¤„ç†æ–°æ ¼å¼
                if isinstance(parsed, dict) and "plans" in parsed:
                    parsed = parsed["plans"]
                if isinstance(parsed, list):
                    for plan in parsed:
                        total_plans += 1
                        if plan.get("domain") not in valid_domains:
                            hallucination_count += 1
            except Exception:
                pass
    
    hallucination_rate = hallucination_count / total_plans if total_plans > 0 else 0
    
    # 3. è¶…é•¿è¾“å‡ºï¼ˆ> 500 å­—ç¬¦ï¼‰
    long_output_count = (dv["raw_response"].str.len() > 500).sum()
    long_output_rate = long_output_count / len(dv) if len(dv) > 0 else 0
    
    return {
        "variant": variant,
        "failure_rate": failure_rate,
        "hallucination_rate": hallucination_rate,
        "long_output_rate": long_output_rate,
        "total_plans_checked": total_plans
    }


# ============================================================================
# 5. æ€§èƒ½åˆ†æ
# ============================================================================

def analyze_performance(df: pd.DataFrame, variant: str) -> dict[str, Any]:
    """åˆ†ææ€§èƒ½æŒ‡æ ‡
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        variant: prompt å˜ä½“
        
    Returns:
        æ€§èƒ½åˆ†æç»“æœ
    """
    dv = df[df["variant"] == variant]
    valid_latency = dv["latency_sec"].dropna()
    
    return {
        "variant": variant,
        "mean_latency": valid_latency.mean() if len(valid_latency) > 0 else None,
        "median_latency": valid_latency.median() if len(valid_latency) > 0 else None,
        "p95_latency": valid_latency.quantile(0.95) if len(valid_latency) > 0 else None,
        "p99_latency": valid_latency.quantile(0.99) if len(valid_latency) > 0 else None,
        "timeout_rate": (dv["latency_sec"] > 30).sum() / len(dv) if len(dv) > 0 else 0,
        "n_samples": len(valid_latency)
    }


# ============================================================================
# 6. ç»¼åˆè¯„åˆ†
# ============================================================================

def calculate_quality_score(metrics: dict[str, Any]) -> float:
    """è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ† (0-100)
    
    Args:
        metrics: åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        
    Returns:
        è´¨é‡è¯„åˆ†
    """
    # æƒé‡åˆ†é…
    weights = {
        "json_validity": 0.25,
        "exact_match_rate": 0.25,
        "key_field_match_rate": 0.15,
        "failure_rate": 0.15,
        "hallucination_rate": 0.10,
        "diversity_rate": 0.05,
        "refuse_rate_accuracy": 0.05
    }
    
    score = 0.0
    score += metrics.get("json_validity", 0) * weights["json_validity"] * 100
    score += metrics.get("exact_match_rate", 0) * weights["exact_match_rate"] * 100
    score += metrics.get("key_field_match_rate", 0) * weights["key_field_match_rate"] * 100
    score += (1 - metrics.get("failure_rate", 0)) * weights["failure_rate"] * 100
    score += (1 - metrics.get("hallucination_rate", 0)) * weights["hallucination_rate"] * 100
    score += min(metrics.get("diversity_rate", 0), 0.3) / 0.3 * weights["diversity_rate"] * 100
    
    # æ‹’ç­”ç‡å‡†ç¡®åº¦ï¼ˆæ¥è¿‘ 8.4% ä¸ºæœ€ä¼˜ï¼‰
    target_refuse_rate = 0.084
    actual_refuse_rate = metrics.get("refuse_rate", 0)
    refuse_accuracy = 1 - abs(actual_refuse_rate - target_refuse_rate) / target_refuse_rate
    refuse_accuracy = max(0, refuse_accuracy)
    score += refuse_accuracy * weights["refuse_rate_accuracy"] * 100
    
    return score


# ============================================================================
# 7. å®Œæ•´åˆ†ææµç¨‹
# ============================================================================

def full_analysis(csv_path: str, output_dir: str | None = None) -> dict[str, Any]:
    """å®Œæ•´çš„åˆ†ææµç¨‹
    
    Args:
        csv_path: è¯„ä¼°ç»“æœ CSV æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
    """
    logging.info(f"ğŸ“Š å¼€å§‹åˆ†æ prompt æ•ˆæœ")
    logging.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {csv_path}")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(csv_path)
    logging.info(f"ğŸ“ˆ æ•°æ®é›†å¤§å°: {len(df)} è¡Œ")
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = str(Path(csv_path).parent)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åˆ†æç»“æœå®¹å™¨
    results = {}
    
    # å¯¹æ¯ä¸ª variant è¿›è¡Œåˆ†æ
    variants = df["variant"].unique()
    logging.info(f"ğŸ” å‘ç° {len(variants)} ä¸ªå˜ä½“: {', '.join(variants)}")
    
    for variant in variants:
        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸ“Š åˆ†æ {variant.upper()} Prompt")
        logging.info(f"{'='*60}")
        
        # 1. ç»“æ„åŒ–ç¨‹åº¦
        structure_metrics = analyze_structure(df, variant)
        logging.info(f"âœ… ç»“æ„åŒ–åˆ†æå®Œæˆ")
        
        # 2. é‡‘æ ‡ç­¾åŒ¹é…åº¦
        gold_metrics = analyze_gold_match(df, variant)
        logging.info(f"âœ… é‡‘æ ‡ç­¾åŒ¹é…åˆ†æå®Œæˆ")
        
        # 3. å¤šæ ·æ€§
        diversity_metrics = analyze_diversity(df, variant)
        logging.info(f"âœ… å¤šæ ·æ€§åˆ†æå®Œæˆ")
        
        # 4. é²æ£’æ€§
        robustness_metrics = analyze_robustness(df, variant)
        logging.info(f"âœ… é²æ£’æ€§åˆ†æå®Œæˆ")
        
        # 5. æ€§èƒ½
        performance_metrics = analyze_performance(df, variant)
        logging.info(f"âœ… æ€§èƒ½åˆ†æå®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            **structure_metrics,
            **gold_metrics,
            **diversity_metrics,
            **robustness_metrics,
            **performance_metrics
        }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        quality_score = calculate_quality_score(all_metrics)
        all_metrics["quality_score"] = quality_score
        
        results[variant] = all_metrics
        
        logging.info(f"ğŸ¯ ç»¼åˆè´¨é‡è¯„åˆ†: {quality_score:.2f}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    logging.info(f"\n{'='*60}")
    logging.info(f"ğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
    logging.info(f"{'='*60}")
    
    # ä¿å­˜ JSON æŠ¥å‘Š
    json_path = output_path / "analysis_report.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logging.info(f"ğŸ’¾ JSON æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_report = generate_markdown_report(results)
    md_path = output_path / "analysis_report.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_report)
    logging.info(f"ğŸ’¾ Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")
    
    # æ‰“å°ç®€è¦å¯¹æ¯”
    print_comparison_summary(results)
    
    return results


def generate_markdown_report(results: dict[str, Any]) -> str:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š
    
    Args:
        results: åˆ†æç»“æœ
        
    Returns:
        Markdown æ ¼å¼çš„æŠ¥å‘Š
    """
    lines = []
    lines.append("# Prompt æ•ˆæœåˆ†ææŠ¥å‘Š")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # è´¨é‡è¯„åˆ†å¯¹æ¯”
    lines.append("## ğŸ“Š ç»¼åˆè´¨é‡è¯„åˆ†")
    lines.append("")
    for variant, metrics in results.items():
        score = metrics.get("quality_score", 0)
        emoji = "âœ…" if score >= 80 else "âš ï¸" if score >= 70 else "âŒ"
        lines.append(f"- **{variant.upper()} Prompt**: {score:.2f} åˆ† {emoji}")
    lines.append("")
    
    # è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”
    lines.append("## ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”")
    lines.append("")
    lines.append("| æŒ‡æ ‡ | " + " | ".join([v.upper() for v in results.keys()]) + " | æœ€ä¼˜ |")
    lines.append("|------|" + "------|" * len(results) + "------|")
    
    # å®šä¹‰è¦å¯¹æ¯”çš„æŒ‡æ ‡
    metrics_to_compare = [
        ("JSON æœ‰æ•ˆæ€§", "json_validity", "percent", "higher"),
        ("å­—æ®µå®Œæ•´æ€§", "avg_completeness", "percent", "higher"),
        ("ç²¾ç¡®åŒ¹é…ç‡", "exact_match_rate", "percent", "higher"),
        ("å…³é”®å­—æ®µåŒ¹é…ç‡", "key_field_match_rate", "percent", "higher"),
        ("å¹³å‡ç›¸ä¼¼åº¦", "avg_similarity", "percent", "higher"),
        ("å¤±è´¥ç‡", "failure_rate", "percent", "lower"),
        ("å¹»è§‰ç‡", "hallucination_rate", "percent", "lower"),
        ("è¶…é•¿è¾“å‡ºç‡", "long_output_rate", "percent", "lower"),
        ("æ‹’ç­”ç‡", "refuse_rate", "percent", "target_8.4"),
        ("å¤šæ ·æ€§ç‡", "diversity_rate", "percent", "moderate"),
        ("å¹³å‡è®¡åˆ’é•¿åº¦", "avg_plan_length", "number", "neutral"),
        ("å¹³å‡å»¶è¿Ÿ(ç§’)", "mean_latency", "number", "lower"),
        ("P95 å»¶è¿Ÿ(ç§’)", "p95_latency", "number", "lower"),
    ]
    
    for label, key, fmt, direction in metrics_to_compare:
        row = [label]
        values = []
        for variant in results.keys():
            val = results[variant].get(key)
            if val is None:
                row.append("N/A")
                values.append(None)
            elif fmt == "percent":
                row.append(f"{val*100:.1f}%")
                values.append(val)
            else:
                row.append(f"{val:.2f}")
                values.append(val)
        
        # ç¡®å®šæœ€ä¼˜
        valid_values = [(i, v) for i, v in enumerate(values) if v is not None]
        if valid_values:
            if direction == "higher":
                best_idx = max(valid_values, key=lambda x: x[1])[0]
            elif direction == "lower":
                best_idx = min(valid_values, key=lambda x: x[1])[0]
            elif direction == "target_8.4":
                best_idx = min(valid_values, key=lambda x: abs(x[1] - 0.084))[0]
            else:
                best_idx = -1
            
            if best_idx >= 0:
                best_variant = list(results.keys())[best_idx]
                row.append(f"âœ… {best_variant.upper()}")
            else:
                row.append("â–")
        else:
            row.append("N/A")
        
        lines.append("| " + " | ".join(row) + " |")
    
    lines.append("")
    
    # æ€§èƒ½ç»Ÿè®¡
    lines.append("## âš¡ æ€§èƒ½ç»Ÿè®¡")
    lines.append("")
    for variant, metrics in results.items():
        lines.append(f"### {variant.upper()} Prompt")
        lines.append("")
        lines.append(f"- å¹³å‡å»¶è¿Ÿ: {metrics.get('mean_latency', 0):.2f}s")
        lines.append(f"- ä¸­ä½æ•°å»¶è¿Ÿ: {metrics.get('median_latency', 0):.2f}s")
        lines.append(f"- P95 å»¶è¿Ÿ: {metrics.get('p95_latency', 0):.2f}s")
        lines.append(f"- P99 å»¶è¿Ÿ: {metrics.get('p99_latency', 0):.2f}s")
        lines.append(f"- è¶…æ—¶ç‡: {metrics.get('timeout_rate', 0)*100:.1f}%")
        lines.append("")
    
    # ç»“è®º
    lines.append("## ğŸ¯ ç»“è®º")
    lines.append("")
    
    # æ‰¾å‡ºå¾—åˆ†æœ€é«˜çš„
    best_variant = max(results.items(), key=lambda x: x[1].get("quality_score", 0))
    lines.append(f"**æœ€ä¼˜ Prompt**: {best_variant[0].upper()}")
    lines.append(f"**ç»¼åˆè¯„åˆ†**: {best_variant[1].get('quality_score', 0):.2f}")
    lines.append("")
    
    # å…³é”®ä¼˜åŠ¿
    lines.append("### å…³é”®ä¼˜åŠ¿")
    lines.append("")
    best_metrics = best_variant[1]
    if best_metrics.get("json_validity", 0) > 0.95:
        lines.append("- âœ… JSON æœ‰æ•ˆæ€§ä¼˜ç§€ (>95%)")
    if best_metrics.get("exact_match_rate", 0) > 0.70:
        lines.append("- âœ… ç²¾ç¡®åŒ¹é…ç‡ä¼˜ç§€ (>70%)")
    if best_metrics.get("failure_rate", 0) < 0.05:
        lines.append("- âœ… å¤±è´¥ç‡ä½ (<5%)")
    if best_metrics.get("hallucination_rate", 0) < 0.01:
        lines.append("- âœ… å‡ ä¹æ— å¹»è§‰è¾“å‡º")
    
    lines.append("")
    
    return "\n".join(lines)


def print_comparison_summary(results: dict[str, Any]) -> None:
    """æ‰“å°ç®€è¦å¯¹æ¯”æ‘˜è¦
    
    Args:
        results: åˆ†æç»“æœ
    """
    print("\n" + "="*60)
    print("ğŸ“Š Prompt æ•ˆæœå¯¹æ¯”æ‘˜è¦")
    print("="*60)
    
    for variant, metrics in results.items():
        print(f"\nğŸ”· {variant.upper()} Prompt:")
        print(f"   ç»¼åˆè¯„åˆ†: {metrics.get('quality_score', 0):.2f} åˆ†")
        print(f"   JSONæœ‰æ•ˆæ€§: {metrics.get('json_validity', 0)*100:.1f}%")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {metrics.get('exact_match_rate', 0)*100:.1f}%")
        print(f"   å¤±è´¥ç‡: {metrics.get('failure_rate', 0)*100:.1f}%")
        print(f"   å¹³å‡å»¶è¿Ÿ: {metrics.get('mean_latency', 0):.2f}s")
    
    print("\n" + "="*60)


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = ArgumentParser(description="Prompt æ•ˆæœåˆ†æè„šæœ¬")
    parser.add_argument(
        "csv_path",
        help="è¯„ä¼°ç»“æœ CSV æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--outdir",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰",
        default=None
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.csv_path).exists():
        logging.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_path}")
        sys.exit(1)
    
    try:
        # è¿è¡Œå®Œæ•´åˆ†æ
        results = full_analysis(args.csv_path, args.outdir)
        
        logging.info("\nâœ… åˆ†æå®Œæˆï¼")
        logging.info("ğŸ“ è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
        output_dir = args.outdir or str(Path(args.csv_path).parent)
        logging.info(f"   - {output_dir}/analysis_report.json")
        logging.info(f"   - {output_dir}/analysis_report.md")
        
    except Exception as e:
        logging.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()

