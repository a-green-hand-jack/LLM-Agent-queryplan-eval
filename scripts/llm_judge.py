#!/usr/bin/env python3
"""åŸºäº LLM çš„ Prompt æ•ˆæœåˆ¤åˆ«è„šæœ¬

ä½¿ç”¨ qwen3-max æ¨¡å‹ä½œä¸ºè¯„åˆ¤å™¨ï¼Œå¯¹æ¯” new å’Œ old prompt çš„è¾“å‡ºè´¨é‡ã€‚

ä½¿ç”¨æ–¹å¼:
    uv run python scripts/llm_judge.py outputs/v4/eval_results.csv
    uv run python scripts/llm_judge.py outputs/v4/eval_results.csv --model qwen-max
"""

import sys
from pathlib import Path

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))

import os
import json
import time
import logging
from typing import Any, Optional, Tuple
from argparse import ArgumentParser

import pandas as pd
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv

import openai
import outlines

from queryplan_eval.schemas import JudgementResult
from queryplan_eval.renderer import read_raw_prompt

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def build_judge_chat(
    system_prompt: str,
    query: str,
    gold_standard: str,
    candidate_a: str,
    candidate_b: str
) -> list[dict[str, Any]]:
    """æ„å»ºåˆ¤åˆ«è¯·æ±‚çš„èŠå¤©æ¶ˆæ¯
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤º
        query: ç”¨æˆ·æŸ¥è¯¢
        gold_standard: é‡‘æ ‡å‡†ç­”æ¡ˆ
        candidate_a: å€™é€‰Açš„è¾“å‡º
        candidate_b: å€™é€‰Bçš„è¾“å‡º
        
    Returns:
        èŠå¤©æ¶ˆæ¯åˆ—è¡¨
    """
    user_content = f"""è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªå€™é€‰è¾“å‡ºï¼Œåˆ¤æ–­å“ªä¸€ä¸ªæ›´æ¥è¿‘é‡‘æ ‡å‡†ç­”æ¡ˆã€‚

            ## ç”¨æˆ·æŸ¥è¯¢
            ```
            {query}
            ```

            ## é‡‘æ ‡å‡†ç­”æ¡ˆ
            ```json
            {gold_standard}
            ```

            ## å€™é€‰A è¾“å‡º
            ```json
            {candidate_a}
            ```

            ## å€™é€‰B è¾“å‡º
            ```json
            {candidate_b}
            ```

            è¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­çš„è¯„ä¼°æ ‡å‡†ï¼Œç»™å‡ºä½ çš„åˆ¤æ–­ç»“æœã€‚
        """
    
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]


def call_judge_model(
    model: Any,
    chat: list[dict[str, Any]],
    temperature: float = 0.0
) -> Tuple[Optional[JudgementResult], Optional[str], float]:
    """è°ƒç”¨åˆ¤åˆ«æ¨¡å‹
    
    Args:
        model: Outlines åŒ…è£…çš„æ¨¡å‹
        chat: èŠå¤©æ¶ˆæ¯
        temperature: é‡‡æ ·æ¸©åº¦
        
    Returns:
        (è§£æåçš„åˆ¤åˆ«ç»“æœ, åŸå§‹å“åº”, å»¶è¿Ÿ)
    """
    t0 = time.time()
    try:
        result = model(
            outlines.inputs.Chat(chat),
            JudgementResult,
            temperature=temperature
        )
        dt = time.time() - t0
        
        # å¤„ç†ç»“æœ
        parsed: Optional[JudgementResult] = None
        if isinstance(result, str):
            raw = result
            try:
                if hasattr(JudgementResult, "model_validate_json"):
                    parsed = JudgementResult.model_validate_json(raw)
            except Exception as e:
                logging.debug(f"è§£æåˆ¤åˆ«ç»“æœå¤±è´¥: {str(e)}")
                parsed = None
        else:
            raw = json.dumps(result, ensure_ascii=False)
            parsed = result
        
        return parsed, raw, dt
    except Exception as e:
        dt = time.time() - t0
        logging.error(f"è°ƒç”¨åˆ¤åˆ«æ¨¡å‹å¤±è´¥: {str(e)}")
        return None, None, dt


def load_eval_results(csv_path: str) -> pd.DataFrame:
    """åŠ è½½è¯„ä¼°ç»“æœ
    
    Args:
        csv_path: CSV æ–‡ä»¶è·¯å¾„
        
    Returns:
        è¯„ä¼°ç»“æœ DataFrame
    """
    df = pd.read_csv(csv_path)
    logging.info(f"ğŸ“Š åŠ è½½è¯„ä¼°ç»“æœ: {len(df)} è¡Œ")
    return df


def prepare_judgement_pairs(df: pd.DataFrame) -> list[dict[str, Any]]:
    """å‡†å¤‡éœ€è¦åˆ¤åˆ«çš„æ ·æœ¬å¯¹
    
    Args:
        df: è¯„ä¼°ç»“æœ DataFrame
        
    Returns:
        æ ·æœ¬å¯¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« query, gold_label, new_response, old_response
    """
    # å°†æ•°æ®æŒ‰ idx åˆ†ç»„
    grouped = df.groupby("idx")
    
    pairs = []
    for idx, group in grouped:
        # ç¡®ä¿æœ‰ new å’Œ old ä¸¤ä¸ªç‰ˆæœ¬
        variants = group["variant"].unique()
        if len(variants) < 2:
            logging.warning(f"è·³è¿‡ idx={idx}ï¼Œç¼ºå°‘å®Œæ•´çš„å˜ä½“æ•°æ®")
            continue
        
        # æå–æ•°æ®
        new_row = group[group["variant"] == "new"].iloc[0]
        old_row = group[group["variant"] == "old"].iloc[0]
        
        # è·³è¿‡ä¸¤è€…éƒ½å¤±è´¥çš„æƒ…å†µ
        if not new_row["ok"] and not old_row["ok"]:
            logging.debug(f"è·³è¿‡ idx={idx}ï¼Œä¸¤ä¸ªå€™é€‰éƒ½å¤±è´¥")
            continue
        
        pairs.append({
            "idx": int(idx),
            "query": str(new_row["query"]),
            "gold_label": str(new_row["gold_label"]) if pd.notna(new_row["gold_label"]) else "REFUSE",
            "new_response": str(new_row["raw_response"]) if pd.notna(new_row["raw_response"]) else "{}",
            "old_response": str(old_row["raw_response"]) if pd.notna(old_row["raw_response"]) else "{}",
            "new_ok": bool(new_row["ok"]),
            "old_ok": bool(old_row["ok"])
        })
    
    logging.info(f"ğŸ” å‡†å¤‡åˆ¤åˆ«æ ·æœ¬å¯¹: {len(pairs)} å¯¹")
    return pairs


def run_judgements(
    pairs: list[dict[str, Any]],
    model: Any,
    system_prompt: str,
    temperature: float = 0.0
) -> list[dict[str, Any]]:
    """è¿è¡Œ LLM åˆ¤åˆ«
    
    Args:
        pairs: æ ·æœ¬å¯¹åˆ—è¡¨
        model: åˆ¤åˆ«æ¨¡å‹
        system_prompt: ç³»ç»Ÿæç¤º
        temperature: é‡‡æ ·æ¸©åº¦
        
    Returns:
        åˆ¤åˆ«ç»“æœåˆ—è¡¨
    """
    results = []
    
    for pair in tqdm(pairs, desc="LLM åˆ¤åˆ«ä¸­"):
        # æ„å»ºèŠå¤©æ¶ˆæ¯ (candidate_a = new, candidate_b = old)
        chat = build_judge_chat(
            system_prompt,
            pair["query"],
            pair["gold_label"],
            pair["new_response"],
            pair["old_response"]
        )
        
        # è°ƒç”¨åˆ¤åˆ«æ¨¡å‹
        parsed, raw, dt = call_judge_model(model, chat, temperature)
        
        # è®°å½•ç»“æœ
        result = {
            "idx": pair["idx"],
            "query": pair["query"],
            "winner": parsed.winner if parsed else "error",
            "confidence": parsed.confidence if parsed else 0.0,
            "reason": parsed.reason if parsed else "åˆ¤åˆ«å¤±è´¥",
            "latency_sec": dt,
            "raw_judgement": raw
        }
        
        # æ·»åŠ ç»´åº¦å¾—åˆ†
        if parsed and parsed.dimensions:
            dims = parsed.dimensions
            result.update({
                "structure_a_new": dims.structure_a,
                "structure_b_old": dims.structure_b,
                "semantic_a_new": dims.semantic_a,
                "semantic_b_old": dims.semantic_b,
                "completeness_a_new": dims.completeness_a,
                "completeness_b_old": dims.completeness_b,
                "format_a_new": dims.format_a,
                "format_b_old": dims.format_b
            })
        else:
            # å¡«å……é»˜è®¤å€¼
            result.update({
                "structure_a_new": 0.0,
                "structure_b_old": 0.0,
                "semantic_a_new": 0.0,
                "semantic_b_old": 0.0,
                "completeness_a_new": 0.0,
                "completeness_b_old": 0.0,
                "format_a_new": 0.0,
                "format_b_old": 0.0
            })
        
        results.append(result)
        
        # å»¶è¿Ÿé¿å…è¿‡è½½
        time.sleep(0.1)
    
    return results


def analyze_judgement_results(results: list[dict[str, Any]]) -> dict[str, Any]:
    """åˆ†æåˆ¤åˆ«ç»“æœ
    
    Args:
        results: åˆ¤åˆ«ç»“æœåˆ—è¡¨
        
    Returns:
        åˆ†æç»Ÿè®¡
    """
    df = pd.DataFrame(results)
    
    # ç»Ÿè®¡èƒœè´Ÿ
    new_wins = (df["winner"] == "candidate_a").sum()
    old_wins = (df["winner"] == "candidate_b").sum()
    ties = (df["winner"] == "tie").sum()
    errors = (df["winner"] == "error").sum()
    
    total_valid = new_wins + old_wins + ties
    
    # å¹³å‡ç½®ä¿¡åº¦
    avg_confidence = df["confidence"].mean()
    
    # é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence >= 0.7ï¼‰
    high_conf = df[df["confidence"] >= 0.7]
    high_conf_new_wins = (high_conf["winner"] == "candidate_a").sum()
    high_conf_old_wins = (high_conf["winner"] == "candidate_b").sum()
    
    # å„ç»´åº¦å¹³å‡å¾—åˆ†
    dim_scores = {
        "new_structure": df["structure_a_new"].mean(),
        "old_structure": df["structure_b_old"].mean(),
        "new_semantic": df["semantic_a_new"].mean(),
        "old_semantic": df["semantic_b_old"].mean(),
        "new_completeness": df["completeness_a_new"].mean(),
        "old_completeness": df["completeness_b_old"].mean(),
        "new_format": df["format_a_new"].mean(),
        "old_format": df["format_b_old"].mean()
    }
    
    # ç»¼åˆå¾—åˆ† (åŠ æƒ)
    weights = {
        "structure": 0.3,
        "semantic": 0.4,
        "completeness": 0.2,
        "format": 0.1
    }
    
    new_overall = (
        dim_scores["new_structure"] * weights["structure"] +
        dim_scores["new_semantic"] * weights["semantic"] +
        dim_scores["new_completeness"] * weights["completeness"] +
        dim_scores["new_format"] * weights["format"]
    )
    
    old_overall = (
        dim_scores["old_structure"] * weights["structure"] +
        dim_scores["old_semantic"] * weights["semantic"] +
        dim_scores["old_completeness"] * weights["completeness"] +
        dim_scores["old_format"] * weights["format"]
    )
    
    return {
        "total_judgements": len(df),
        "new_wins": int(new_wins),
        "old_wins": int(old_wins),
        "ties": int(ties),
        "errors": int(errors),
        "new_win_rate": new_wins / total_valid if total_valid > 0 else 0,
        "old_win_rate": old_wins / total_valid if total_valid > 0 else 0,
        "tie_rate": ties / total_valid if total_valid > 0 else 0,
        "avg_confidence": float(avg_confidence),
        "high_conf_judgements": len(high_conf),
        "high_conf_new_wins": int(high_conf_new_wins),
        "high_conf_old_wins": int(high_conf_old_wins),
        "dimension_scores": dim_scores,
        "new_overall_score": float(new_overall),
        "old_overall_score": float(old_overall)
    }


def generate_report(
    analysis: dict[str, Any],
    results_df: pd.DataFrame,
    output_dir: Path
) -> None:
    """ç”Ÿæˆåˆ¤åˆ«æŠ¥å‘Š
    
    Args:
        analysis: åˆ†æç»Ÿè®¡
        results_df: åˆ¤åˆ«ç»“æœ DataFrame
        output_dir: è¾“å‡ºç›®å½•
    """
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = output_dir / "llm_judgement_results.csv"
    results_df.to_csv(results_path, index=False, encoding='utf-8')
    logging.info(f"ğŸ’¾ è¯¦ç»†åˆ¤åˆ«ç»“æœå·²ä¿å­˜: {results_path}")
    
    # ä¿å­˜ç»Ÿè®¡åˆ†æ
    analysis_path = output_dir / "llm_judgement_analysis.json"
    with open(analysis_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    logging.info(f"ğŸ’¾ ç»Ÿè®¡åˆ†æå·²ä¿å­˜: {analysis_path}")
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_lines = []
    md_lines.append("# LLM åˆ¤åˆ«æŠ¥å‘Š")
    md_lines.append("")
    md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_lines.append("")
    
    md_lines.append("## ğŸ“Š èƒœè´Ÿç»Ÿè®¡")
    md_lines.append("")
    md_lines.append(f"- æ€»åˆ¤åˆ«æ•°: {analysis['total_judgements']}")
    md_lines.append(f"- **NEW èƒœå‡º**: {analysis['new_wins']} ({analysis['new_win_rate']*100:.1f}%)")
    md_lines.append(f"- **OLD èƒœå‡º**: {analysis['old_wins']} ({analysis['old_win_rate']*100:.1f}%)")
    md_lines.append(f"- **å¹³å±€**: {analysis['ties']} ({analysis['tie_rate']*100:.1f}%)")
    md_lines.append(f"- åˆ¤åˆ«å¤±è´¥: {analysis['errors']}")
    md_lines.append("")
    
    md_lines.append("## ğŸ¯ é«˜ç½®ä¿¡åº¦åˆ¤åˆ«ï¼ˆconfidence â‰¥ 0.7ï¼‰")
    md_lines.append("")
    md_lines.append(f"- é«˜ç½®ä¿¡åˆ¤åˆ«æ•°: {analysis['high_conf_judgements']}")
    md_lines.append(f"- NEW èƒœå‡º: {analysis['high_conf_new_wins']}")
    md_lines.append(f"- OLD èƒœå‡º: {analysis['high_conf_old_wins']}")
    md_lines.append("")
    
    md_lines.append("## ğŸ“ˆ ç»´åº¦å¾—åˆ†å¯¹æ¯”ï¼ˆæ»¡åˆ†10åˆ†ï¼‰")
    md_lines.append("")
    md_lines.append("| ç»´åº¦ | NEW | OLD | å·®å¼‚ | ä¼˜åŠ¿æ–¹ |")
    md_lines.append("|------|-----|-----|------|--------|")
    
    dims = analysis["dimension_scores"]
    dim_pairs = [
        ("ç»“æ„å®Œæ•´æ€§", "new_structure", "old_structure"),
        ("è¯­ä¹‰å‡†ç¡®æ€§", "new_semantic", "old_semantic"),
        ("ä¿¡æ¯å®Œæ•´åº¦", "new_completeness", "old_completeness"),
        ("æ ¼å¼è§„èŒƒæ€§", "new_format", "old_format")
    ]
    
    for label, new_key, old_key in dim_pairs:
        new_val = dims[new_key]
        old_val = dims[old_key]
        diff = new_val - old_val
        winner = "âœ… NEW" if diff > 0.5 else "âœ… OLD" if diff < -0.5 else "â– ç›¸å½“"
        md_lines.append(f"| {label} | {new_val:.2f} | {old_val:.2f} | {diff:+.2f} | {winner} |")
    
    md_lines.append("")
    
    md_lines.append("## ğŸ† ç»¼åˆå¾—åˆ†")
    md_lines.append("")
    md_lines.append(f"- **NEW Prompt**: {analysis['new_overall_score']:.2f} åˆ†")
    md_lines.append(f"- **OLD Prompt**: {analysis['old_overall_score']:.2f} åˆ†")
    md_lines.append(f"- **å·®å¼‚**: {analysis['new_overall_score'] - analysis['old_overall_score']:+.2f} åˆ†")
    md_lines.append("")
    
    md_lines.append("## ğŸ’¡ ç»“è®º")
    md_lines.append("")
    if analysis['new_win_rate'] > 0.6:
        md_lines.append(f"âœ… **NEW Prompt æ˜¾è‘—ä¼˜äº OLD Prompt**")
        md_lines.append(f"   - èƒœç‡: {analysis['new_win_rate']*100:.1f}%")
        md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['new_overall_score']:.2f} vs {analysis['old_overall_score']:.2f}")
    elif analysis['old_win_rate'] > 0.6:
        md_lines.append(f"âŒ **OLD Prompt ä¼˜äº NEW Prompt**")
        md_lines.append(f"   - èƒœç‡: {analysis['old_win_rate']*100:.1f}%")
        md_lines.append(f"   - ç»¼åˆå¾—åˆ†: {analysis['old_overall_score']:.2f} vs {analysis['new_overall_score']:.2f}")
    else:
        md_lines.append(f"â– **ä¸¤è€…è¡¨ç°ç›¸å½“**")
        md_lines.append(f"   - NEW èƒœç‡: {analysis['new_win_rate']*100:.1f}%")
        md_lines.append(f"   - OLD èƒœç‡: {analysis['old_win_rate']*100:.1f}%")
    
    md_lines.append("")
    md_lines.append(f"å¹³å‡ç½®ä¿¡åº¦: {analysis['avg_confidence']:.2f}")
    md_lines.append("")
    
    # ä¿å­˜ Markdown æŠ¥å‘Š
    md_path = output_dir / "llm_judgement_report.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(md_lines))
    logging.info(f"ğŸ’¾ Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")


def print_summary(analysis: dict[str, Any]) -> None:
    """æ‰“å°æ‘˜è¦
    
    Args:
        analysis: åˆ†æç»Ÿè®¡
    """
    print("\n" + "="*60)
    print("ğŸ“Š LLM åˆ¤åˆ«æ‘˜è¦")
    print("="*60)
    print(f"æ€»åˆ¤åˆ«æ•°: {analysis['total_judgements']}")
    print(f"NEW èƒœå‡º: {analysis['new_wins']} ({analysis['new_win_rate']*100:.1f}%)")
    print(f"OLD èƒœå‡º: {analysis['old_wins']} ({analysis['old_win_rate']*100:.1f}%)")
    print(f"å¹³å±€: {analysis['ties']} ({analysis['tie_rate']*100:.1f}%)")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {analysis['avg_confidence']:.2f}")
    print(f"\nç»¼åˆå¾—åˆ†:")
    print(f"  NEW: {analysis['new_overall_score']:.2f}")
    print(f"  OLD: {analysis['old_overall_score']:.2f}")
    print("="*60)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = ArgumentParser(description="åŸºäº LLM çš„ Prompt æ•ˆæœåˆ¤åˆ«")
    parser.add_argument(
        "csv_path",
        help="è¯„ä¼°ç»“æœ CSV æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--model",
        default="qwen3-max",
        help="åˆ¤åˆ«æ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen-maxï¼‰"
    )
    parser.add_argument(
        "--base-url",
        default=os.environ.get(
            "QWEN_BASE_URL",
            "https://dashscope.aliyuncs.com/compatible-mode/v1"
        ),
        help="æ¨¡å‹ API base URL"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤: 0.0ï¼‰"
    )
    parser.add_argument(
        "--outdir",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰"
    )
    parser.add_argument(
        "--judge-prompt",
        default=str(
            Path(__file__).resolve().parents[1]
            / "src"
            / "queryplan_eval"
            / "prompts"
            / "judge_system_prompt.j2"
        ),
        help="åˆ¤åˆ«ç³»ç»Ÿæç¤ºæ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_key = os.environ.get("qwen_key") or os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise SystemExit("âŒ ç¼ºå°‘ API keyã€‚è¯·åœ¨ .env ä¸­è®¾ç½® qwen_key æˆ– OPENAI_API_KEY")
    
    # æ£€æŸ¥æ–‡ä»¶
    if not Path(args.csv_path).exists():
        logging.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_path}")
        sys.exit(1)
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    output_dir = Path(args.outdir) if args.outdir else Path(args.csv_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logging.info(f"ğŸš€ å¼€å§‹ LLM åˆ¤åˆ«")
    logging.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {args.csv_path}")
    logging.info(f"ğŸ¤– åˆ¤åˆ«æ¨¡å‹: {args.model}")
    logging.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        client = openai.OpenAI(base_url=args.base_url, api_key=api_key)
        model = outlines.from_openai(client, args.model)
        logging.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # åŠ è½½åˆ¤åˆ«æç¤º
        judge_prompt = read_raw_prompt(args.judge_prompt)
        logging.info(f"âœ… åˆ¤åˆ«æç¤ºåŠ è½½æˆåŠŸ")
        
        # åŠ è½½è¯„ä¼°ç»“æœ
        df = load_eval_results(args.csv_path)
        
        # å‡†å¤‡åˆ¤åˆ«æ ·æœ¬å¯¹
        pairs = prepare_judgement_pairs(df)
        
        if not pairs:
            logging.error("âŒ æ²¡æœ‰å¯åˆ¤åˆ«çš„æ ·æœ¬å¯¹")
            sys.exit(1)
        
        # è¿è¡Œåˆ¤åˆ«
        results = run_judgements(pairs, model, judge_prompt, args.temperature)
        
        # åˆ†æç»“æœ
        results_df = pd.DataFrame(results)
        analysis = analyze_judgement_results(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report(analysis, results_df, output_dir)
        
        # æ‰“å°æ‘˜è¦
        print_summary(analysis)
        
        logging.info("\nâœ… LLM åˆ¤åˆ«å®Œæˆï¼")
        logging.info("ğŸ“ è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
        logging.info(f"   - {output_dir}/llm_judgement_results.csv")
        logging.info(f"   - {output_dir}/llm_judgement_analysis.json")
        logging.info(f"   - {output_dir}/llm_judgement_report.md")
        
    except Exception as e:
        logging.error(f"âŒ åˆ¤åˆ«è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()

